âœ… FINALIZED PIPELINE (V3)
Runs from hosting/inference server (via cron)

ğŸ” Step-by-Step
Start training VM

Use cloud CLI (az vm start ... or similar)

Prepare training data (locally)

Run export_training_data.py

Export to: models/training/data/new_data/
(leaves existing data untouched)

Transfer new training data â†’ training server

Push: models/training/data/new_data/ â†’ training_server:~/bookrec/models/training/data/

Train models on training server (remote execution):

train_subject_embs.py

train_als.py

precompute_bayesian.py

precompute_embs.py âœ… â† NEW STEP

train_warm_gbt.py

train_cold_gbt.py

Log all stdout and stderr

Transfer trained models back

Pull: training_server:~/bookrec/models/data/ â†’ models/data/

Move new_data/ to main data path

Overwrite: models/training/data/ with new_data/

Optionally delete new_data/

Reload singleton

FastAPI call or local Python call

âœ… Design Clarifications Implemented
âœ… new_data/ ensures no conflict with running inference.

âœ… precompute_embs.py now runs before GBT training.

âœ… All training occurs remotely.

âœ… All movement and updates are atomic from inference side.